# ğŸ“˜ TraduÃ§Ã£o AutomÃ¡tica â€“ Estudo de Caso (D2L, SeÃ§Ã£o 9.5)

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o da seÃ§Ã£o **9.5 â€“ Estudo de Caso de TraduÃ§Ã£o AutomÃ¡tica** do livro [*Dive into Deep Learning*](https://pt.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html).  

O notebook [`Pond_traduÃ§Ã£o_automÃ¡tica.ipynb`](./Pond_traduÃ§Ã£o_automÃ¡tica.ipynb) realiza:

- **Download e prÃ©-processamento** do conjunto de dados do Projeto Tatoeba (inglÃªsâ€“francÃªs).  
- **TokenizaÃ§Ã£o** e construÃ§Ã£o dos vocabulÃ¡rios.  
- ImplementaÃ§Ã£o e **treinamento de um modelo Seq2Seq com atenÃ§Ã£o**.  
- **TraduÃ§Ã£o de frases de exemplo** apÃ³s o treinamento.  
- **Experimentos variando `num_examples`** para analisar o impacto nos vocabulÃ¡rios.  

---

## â“ QuestÃµes da seÃ§Ã£o 9.5.7

### 1. Como a variaÃ§Ã£o do argumento `num_examples` em `load_data_nmt` afeta os tamanhos do vocabulÃ¡rio de origem e destino?

Ao variar `num_examples` (500, 1000, 2000, 6000), observou-se que os tamanhos dos vocabulÃ¡rios aumentaram progressivamente.  
Isso acontece porque, com mais exemplos, aparecem mais palavras Ãºnicas nas sentenÃ§as, ampliando tanto o vocabulÃ¡rio do idioma de origem (inglÃªs) quanto o do idioma de destino (francÃªs).  

ğŸ“Œ **ConclusÃ£o:** mais exemplos = vocabulÃ¡rio mais rico e extenso.

---

### 2. Em idiomas como chinÃªs e japonÃªs, onde nÃ£o hÃ¡ separadores explÃ­citos de palavras, a tokenizaÃ§Ã£o em nÃ­vel de palavra ainda Ã© uma boa ideia?

NÃ£o. Em chinÃªs e japonÃªs nÃ£o existem espaÃ§os delimitando palavras, o que torna a tokenizaÃ§Ã£o em nÃ­vel de palavra ambÃ­gua e pouco confiÃ¡vel.  

Nesses casos, tÃ©cnicas mais adequadas sÃ£o:
- **TokenizaÃ§Ã£o em nÃ­vel de caractere**, onde cada ideograma Ã© tratado como um token.  
- **TokenizaÃ§Ã£o em nÃ­vel de subpalavra** (como BPE ou SentencePiece), que equilibra a cobertura do vocabulÃ¡rio e a generalizaÃ§Ã£o.  

ğŸ“š **ReferÃªncias**:  
- SeÃ§Ã£o 9.5 do *Dive into Deep Learning*.  
- Sennrich et al. (2016), *Neural Machine Translation of Rare Words with Subword Units*.  

ğŸ“Œ **ConclusÃ£o:** tokenizar por palavra nÃ£o Ã© uma boa estratÃ©gia nesses idiomas. Subpalavra ou caractere sÃ£o abordagens preferÃ­veis.
